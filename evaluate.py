import torch
import json
from tqdm import tqdm
from torchmetrics.functional import structural_similarity_index_measure as ssim
from torchmetrics.functional import peak_signal_noise_ratio as psnr
from torchmetrics.image.fid import FrechetInceptionDistance

def evaluate(model, dataloader, device, num_samples=None):
    """
    ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p d·ªØ li·ªáu v·ªõi c√°c ch·ªâ s·ªë SSIM, PSNR v√† FID.

    Args:
        model       : m√¥ h√¨nh MADiffTModel ƒë√£ hu·∫•n luy·ªán
        dataloader  : DataLoader cho t·∫≠p validation/test
        device      : thi·∫øt b·ªã t√≠nh to√°n ('cuda' ho·∫∑c 'cpu')
        num_samples : n·∫øu kh√¥ng None, d·ª´ng sau khi x·ª≠ l√Ω num_samples batch

    Tr·∫£ v·ªÅ:
        dict v·ªõi c√°c kh√≥a 'ssim', 'psnr', 'fid'
    """
    model.eval()  # chuy·ªÉn sang ch·∫ø ƒë·ªô ƒë√°nh gi√°, t·∫Øt dropout, batchnorm‚Ä¶
    total_ssim = 0.0
    total_psnr = 0.0
    count = 0

    # Kh·ªüi t·∫°o metric FID
    fid_metric = FrechetInceptionDistance(feature=64).to(device)

    with torch.no_grad():  # kh√¥ng t√≠nh gradient ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
        for batch in tqdm(dataloader, desc="üîç Evaluating"):
            # batch g·ªìm: masked_img, mask, ·∫£nh g·ªëc, embedding caption, filename
            masked_img, mask, target_img, text_embed, _ = batch

            # ƒê∆∞a d·ªØ li·ªáu l√™n device
            masked_img = masked_img.to(device)
            mask       = mask.to(device)
            target_img = target_img.to(device)
            text_embed = text_embed.to(device)

            # Inference v√† clamp v·ªÅ [0,1]
            output = model(masked_img, mask, text_embed).clamp(0, 1)
            target = target_img.clamp(0, 1)

            # T√≠nh SSIM v√† PSNR tr√™n thang ƒë·ªô 0-1
            total_ssim += ssim(output, target, data_range=1.0).item()
            total_psnr += psnr(output, target, data_range=1.0).item()

            # Chuy·ªÉn sang uint8 [0-255] ƒë·ªÉ t√≠nh FID
            out_uint8    = (output * 255).byte()
            target_uint8 = (target * 255).byte()

            fid_metric.update(target_uint8, real=True)   # m·∫´u th·∫≠t
            fid_metric.update(out_uint8,    real=False)  # m·∫´u sinh

            count += 1
            if num_samples and count >= num_samples:
                break

    # Trung b√¨nh c√°c ch·ªâ s·ªë
    avg_ssim = total_ssim / count
    avg_psnr = total_psnr / count
    fid_score = fid_metric.compute().item()  # t√≠nh FID cu·ªëi c√πng
    fid_metric.reset()

    # In k·∫øt qu·∫£
    print("Evaluation Metrics:")
    print(f"   SSIM: {avg_ssim:.4f}")
    print(f"   PSNR: {avg_psnr:.2f} dB")
    print(f"   FID : {fid_score:.2f}")

    return {
        "ssim": avg_ssim,
        "psnr": avg_psnr,
        "fid": fid_score
    }


def save_metrics(metrics, save_path="evaluation_metrics.json", format="json"):
    """
    L∆∞u c√°c ch·ªâ s·ªë ƒë√°nh gi√° ra file JSON ho·∫∑c TXT.

    Args:
        metrics   : dict {'ssim', 'psnr', 'fid'}
        save_path : ƒë∆∞·ªùng d·∫´n file l∆∞u
        format    : 'json' ho·∫∑c 'txt'
    """
    if format == "json":
        # Ghi file JSON v·ªõi indent ƒë·ªÉ ƒë·ªçc d·ªÖ
        with open(save_path, "w") as f:
            json.dump(metrics, f, indent=4)
        print(f"Metrics saved to {save_path} (JSON)")
    elif format == "txt":
        # Ghi file vƒÉn b·∫£n ƒë∆°n gi·∫£n
        with open(save_path, "w") as f:
            f.write("Evaluation Metrics:\n")
            f.write(f"SSIM: {metrics['ssim']:.4f}\n")
            f.write(f"PSNR: {metrics['psnr']:.2f} dB\n")
            f.write(f"FID : {metrics['fid']:.2f}\n")
        print(f"Metrics saved to {save_path} (TXT)")
    else:
        # N·∫øu format kh√¥ng ƒë√∫ng, raise l·ªói
        raise ValueError("format ph·∫£i l√† 'json' ho·∫∑c 'txt'")
